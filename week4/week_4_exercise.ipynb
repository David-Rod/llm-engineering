{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bc30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "# from google import genai\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import types\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ecbcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e8f46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY', 'your-key-if-not-using-env')\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "os.environ['HF_QWEN_URL'] = os.getenv('HF_QWEN_URL', 'your-url-if-not-using-env')\n",
    "os.environ['HF_CODE_GEMMA_URL'] = os.getenv('HF_CODE_GEMMA_URL', 'your-url-if-not-using-env')\n",
    "\n",
    "\n",
    "\n",
    "CODE_QWEN_URL = os.environ['HF_QWEN_URL']\n",
    "CODE_GEMMA_URL = os.environ['HF_CODE_GEMMA_URL']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a9aa874",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
    "\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
    "GEMINI_MODEL = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "genai_model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "code_gemma = \"google/codegemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9418513",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a code translator which translates Python to C++ code. You only output raw code with no formatting, explanations, or markdown. Never use ``` code blocks. \"\n",
    "system_message += \"The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef89c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = \"CRITICAL REQUIREMENT: Respond ONLY with raw C++ code only. No explanations, no markdown formatting, no code blocks, no Python code remnants.\\n\\n\"\n",
    "    user_prompt += \"Task: Rewrite this Python code in C++ with the fastest possible implementation that produces identical output.\\n\\n\"\n",
    "    user_prompt += \"Please include all necessary dependencies such as <iomanip>\\n\\n\"\n",
    "    user_prompt += \"Requirements:\\n\"\n",
    "    user_prompt += \"- Include all necessary #include statements\\n\"\n",
    "    user_prompt += \"- Make sure to include <chrono> if using timing, <iomanip> for formatting, and use std:: prefixes.\\n\"\n",
    "    user_prompt += \"- Add brief inline comments and docstring for clarity\\n\"\n",
    "    user_prompt += \"- NO markdown code blocks (```cpp)\\n\"\n",
    "    user_prompt += \"- NO explanatory text before or after code\\n\"\n",
    "    user_prompt += \"- NO token artifacts like </start_of_turn> or <|im_end|>\\n\\n\"\n",
    "    user_prompt += \"Python code to convert:\\n\"\n",
    "    user_prompt += python\n",
    "    user_prompt += \"\\n\\nOutput format: Raw C++ code starting with #include statements.\"\n",
    "    user_prompt += \"\\n\\nIMPORTANT: Your response should start with #include and contain ONLY C++ code. \"\n",
    "    user_prompt += \"No markdown blocks, no explanations, no ```cpp formatting.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a0beeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemma_user_prompt(message):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(message)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d822009",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_items = [\n",
    "    {'name': 'UPLIFT V2 Standing Desk, 48\" x 30\" Bamboo Desktop', 'quantity': 2, 'price': '599.00'},\n",
    "    {'name': 'Herman Miller Aeron Ergonomic Office Chair, Size B, Graphite', 'quantity': 4, 'price': '1395.00'},\n",
    "    {'name': 'Dell UltraSharp 27\" 4K USB-C Monitor (U2723QE)', 'quantity': 6, 'price': '649.99'},\n",
    "    {'name': 'Amazon Basics Office Supply Bundle - Pens, Notebooks, Folders, Paper', 'quantity': 3, 'price': '49.99'},\n",
    "    {'name': 'Logitech MX Keys Advanced Wireless Illuminated Keyboard', 'quantity': 6, 'price': '99.99'},\n",
    "    {'name': 'Logitech MX Master 3S Advanced Wireless Mouse', 'quantity': 6, 'price': '99.99'},\n",
    "    {'name': 'SteelSeries QcK Gaming Mouse Pad - Cloth Surface (Medium)', 'quantity': 6, 'price': '14.99'},\n",
    "    {'name': 'VIVO Dual Monitor Desk Mount Stand for 13\" to 27\" Screens', 'quantity': 3, 'price': '39.99'},\n",
    "    {'name': 'HON Brigade 4-Drawer Letter-Size File Cabinet, Light Gray', 'quantity': 2, 'price': '319.99'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b956223",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorize_func = '''\n",
    "import time\n",
    "sample_items = [\n",
    "    {'name': 'UPLIFT V2 Standing Desk, 48\" x 30\" Bamboo Desktop', 'quantity': 2, 'price': '599.00'},\n",
    "    {'name': 'Herman Miller Aeron Ergonomic Office Chair, Size B, Graphite', 'quantity': 4, 'price': '1395.00'},\n",
    "    {'name': 'Dell UltraSharp 27\" 4K USB-C Monitor (U2723QE)', 'quantity': 6, 'price': '649.99'},\n",
    "    {'name': 'Amazon Basics Office Supply Bundle - Pens, Notebooks, Folders, Paper', 'quantity': 3, 'price': '49.99'},\n",
    "    {'name': 'Logitech MX Keys Advanced Wireless Illuminated Keyboard', 'quantity': 6, 'price': '99.99'},\n",
    "    {'name': 'Logitech MX Master 3S Advanced Wireless Mouse', 'quantity': 6, 'price': '99.99'},\n",
    "    {'name': 'SteelSeries QcK Gaming Mouse Pad - Cloth Surface (Medium)', 'quantity': 6, 'price': '14.99'},\n",
    "    {'name': 'VIVO Dual Monitor Desk Mount Stand for 13\" to 27\" Screens', 'quantity': 3, 'price': '39.99'},\n",
    "    {'name': 'HON Brigade 4-Drawer Letter-Size File Cabinet, Light Gray', 'quantity': 2, 'price': '319.99'}\n",
    "]\n",
    "\n",
    "def categorize_order_items(items):\n",
    "    category_keywords = {\n",
    "        'Furniture': [\n",
    "            'desk', 'chair', 'table', 'cabinet', 'filing cabinet', 'stand',\n",
    "            'furniture', 'seating', 'workstation', 'storage', 'shelf',\n",
    "            'drawer', 'office furniture', 'ergonomic chair', 'standing desk'\n",
    "        ],\n",
    "        'Electronics': [\n",
    "            'monitor', 'keyboard', 'mouse', 'computer', 'electronic', 'wireless',\n",
    "            'usb', 'digital', 'screen', 'display', 'tech', 'device', 'gaming',\n",
    "            'bluetooth', 'connectivity', 'hardware', 'peripheral'\n",
    "        ],\n",
    "        'Stationary': [\n",
    "            'pen', 'pencil', 'paper', 'notebook', 'folder', 'stationary',\n",
    "            'stationery', 'office supply', 'writing', 'notepad', 'binder',\n",
    "            'clip', 'stapler', 'tape', 'supplies', 'bundle'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    categorized_items = {\n",
    "        'Furniture': [],\n",
    "        'Electronics': [],\n",
    "        'Stationary': []\n",
    "    }\n",
    "\n",
    "\n",
    "    for item in items:\n",
    "\n",
    "        item_name = ''\n",
    "        if isinstance(item, dict):\n",
    "            item_name = (item.get('name', '') or\n",
    "                        item.get('product_name', '') or\n",
    "                        item.get('title', '') or\n",
    "                        str(item)).lower()\n",
    "        else:\n",
    "            item_name = str(item).lower()\n",
    "\n",
    "\n",
    "        category_scores = {}\n",
    "\n",
    "        for category, keywords in category_keywords.items():\n",
    "            score = 0\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in item_name:\n",
    "                    score += len(keyword.split())\n",
    "            category_scores[category] = score\n",
    "\n",
    "\n",
    "        if max(category_scores.values()) > 0:\n",
    "            best_category = max(category_scores, key=category_scores.get)\n",
    "            categorized_items[best_category].append(item)\n",
    "        else:\n",
    "\n",
    "            categorized_items['Stationary'].append(item)\n",
    "\n",
    "    return categorized_items\n",
    "\n",
    "start_time = time.time()\n",
    "result = categorize_order_items(sample_items)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "for category, items in result.items():\n",
    "    print(\"\")  # Empty line before each category\n",
    "    print(\"{} ({} items):\".format(category, len(items)))\n",
    "    for item in items:\n",
    "        name = item[\"name\"]\n",
    "        print(\"  - {}\".format(name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad51317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85aa70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(cpp):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    with open(\"optimized.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "606ec2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(python):\n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e3cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(python):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd07cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini(python):\n",
    "    response = genai_model.generate_content(\n",
    "        user_prompt_for(python),\n",
    "        stream=True\n",
    "    )\n",
    "    reply = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.text:\n",
    "            fragment = chunk.text\n",
    "            reply += fragment\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c1bf080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python(code):\n",
    "    try:\n",
    "        namespace = {\n",
    "            'time': __import__('time'),\n",
    "            'json': __import__('json'),\n",
    "            'sys': __import__('sys'),\n",
    "            'dict': dict,\n",
    "            'list': list,\n",
    "            'str': str,\n",
    "            'max': max,\n",
    "            'len': len,\n",
    "            'isinstance': isinstance,\n",
    "            'print': print,\n",
    "            'enumerate': enumerate\n",
    "        }\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        compiled = compile(code, '<string>', 'exec')\n",
    "        exec(compiled, namespace)\n",
    "        return output.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f9d4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cpp(code):\n",
    "    write_output(code)\n",
    "    compile_cmd = [\"g++\", \"-O3\", \"-std=c++17\", \"-march=x86-64-v3\", \"-mtune=native\", \"-o\", \"optimized\", \"optimized.cpp\"]\n",
    "    try:\n",
    "        compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
    "        run_cmd = [\"./optimized\"]\n",
    "        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b183baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_code_qwen(python):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(code_qwen)\n",
    "    messages = messages_for(python)\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    client = InferenceClient(CODE_QWEN_URL, token=HF_TOKEN)\n",
    "    stream = client.text_generation(text, stream=True, details=True, max_new_tokens=3000)\n",
    "    result = \"\"\n",
    "    for r in stream:\n",
    "        result += r.token.text\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d146dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemma(python):\n",
    "    gemma_tokenizer = AutoTokenizer.from_pretrained(code_gemma)\n",
    "    messages = gemma_user_prompt(python)\n",
    "    text = gemma_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    client = InferenceClient(CODE_QWEN_URL, token=HF_TOKEN)\n",
    "    stream = client.text_generation(text, stream=True, details=True, max_new_tokens=3000)\n",
    "    result = \"\"\n",
    "    for r in stream:\n",
    "        result += r.token.text\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f60b45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response):\n",
    "    response = response.replace(\"```cpp\", \"\")\n",
    "    response = response.replace(\"```\", \"\")\n",
    "    response = response.replace(\"<|im_end|>\", \"\")\n",
    "    response = response.replace(\"</start_of_turn>\", \"\")\n",
    "    response = response.replace(\"<|im_start|>\", \"\")\n",
    "\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "34701fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(python)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(python)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(python)\n",
    "    elif model==\"Qwen2\":\n",
    "        result = stream_code_qwen(python)\n",
    "    elif model==\"Gemma\":\n",
    "        result = stream_gemma(python)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    result = (clean_response(stream_so_far) for stream_so_far in result)\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fed79d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    ".cpp {background-color: #050;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73507c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample_program(sample_program):\n",
    "    if sample_program==\"categorize_items\":\n",
    "        return categorize_func\n",
    "    else:\n",
    "        return \"Type your Python program here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d4fc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "VISUAL_STUDIO_2022_TOOLS = \"C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\Common7\\Tools\\\\VsDevCmd.bat\"\n",
    "VISUAL_STUDIO_2019_TOOLS = \"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\Common7\\\\Tools\\\\VsDevCmd.bat\"\n",
    "\n",
    "simple_cpp = \"\"\"\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"Hello\";\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def run_cmd(command_to_run):\n",
    "    try:\n",
    "        run_result = subprocess.run(command_to_run, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout if run_result.stdout else \"SUCCESS\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def c_compiler_cmd(filename_base):\n",
    "    my_platform = platform.system()\n",
    "    my_compiler = []\n",
    "\n",
    "    try:\n",
    "        with open(\"simple.cpp\", \"w\") as f:\n",
    "            f.write(simple_cpp)\n",
    "\n",
    "        if my_platform == \"Windows\":\n",
    "            if os.path.isfile(VISUAL_STUDIO_2022_TOOLS):\n",
    "                if os.path.isfile(\"./simple.exe\"):\n",
    "                    os.remove(\"./simple.exe\")\n",
    "                compile_cmd = [\"cmd\", \"/c\", VISUAL_STUDIO_2022_TOOLS, \"&\", \"cl\", \"simple.cpp\"]\n",
    "                if run_cmd(compile_cmd):\n",
    "                    if run_cmd([\"./simple.exe\"]) == \"Hello\":\n",
    "                        my_compiler = [\"Windows\", \"Visual Studio 2022\", [\"cmd\", \"/c\", VISUAL_STUDIO_2022_TOOLS, \"&\", \"cl\", f\"{filename_base}.cpp\"]]\n",
    "\n",
    "            if not my_compiler:\n",
    "                if os.path.isfile(VISUAL_STUDIO_2019_TOOLS):\n",
    "                    if os.path.isfile(\"./simple.exe\"):\n",
    "                        os.remove(\"./simple.exe\")\n",
    "                    compile_cmd = [\"cmd\", \"/c\", VISUAL_STUDIO_2019_TOOLS, \"&\", \"cl\", \"simple.cpp\"]\n",
    "                    if run_cmd(compile_cmd):\n",
    "                        if run_cmd([\"./simple.exe\"]) == \"Hello\":\n",
    "                            my_compiler = [\"Windows\", \"Visual Studio 2019\", [\"cmd\", \"/c\", VISUAL_STUDIO_2019_TOOLS, \"&\", \"cl\", f\"{filename_base}.cpp\"]]\n",
    "\n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "\n",
    "        elif my_platform == \"Linux\":\n",
    "            # Try g++ first with x86-64-v3 architecture\n",
    "            if os.path.isfile(\"./simple\"):\n",
    "                os.remove(\"./simple\")\n",
    "            compile_cmd = [\"g++\", \"-O3\", \"-std=c++17\", \"-march=x86-64-v3\", \"-mtune=native\", \"-o\", \"simple\", \"simple.cpp\"]\n",
    "            if run_cmd(compile_cmd):\n",
    "                if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                    my_compiler = [\"Linux\", \"GCC (g++)\", [\"g++\", \"-O3\", \"-std=c++17\", \"-march=x86-64-v3\", \"-mtune=native\", \"-o\", f\"{filename_base}\", f\"{filename_base}.cpp\"]]\n",
    "\n",
    "            # Try clang++ if g++ fails\n",
    "            if not my_compiler:\n",
    "                if os.path.isfile(\"./simple\"):\n",
    "                    os.remove(\"./simple\")\n",
    "                compile_cmd = [\"clang++\", \"-O3\", \"-std=c++17\", \"-march=x86-64-v3\", \"-mtune=native\", \"-o\", \"simple\", \"simple.cpp\"]\n",
    "                if run_cmd(compile_cmd):\n",
    "                    if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                        my_compiler = [\"Linux\", \"Clang++\", [\"clang++\", \"-O3\", \"-std=c++17\", \"-march=x86-64-v3\", \"-mtune=native\", \"-o\", f\"{filename_base}\", f\"{filename_base}.cpp\"]]\n",
    "\n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "\n",
    "        elif my_platform == \"Darwin\":\n",
    "            if os.path.isfile(\"./simple\"):\n",
    "                os.remove(\"./simple\")\n",
    "            compile_cmd = [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", \"simple\", \"simple.cpp\"]\n",
    "            if run_cmd(compile_cmd):\n",
    "                if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                    my_compiler = [\"Macintosh\", \"Clang++\", [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", f\"{filename_base}\", f\"{filename_base}.cpp\"]]\n",
    "\n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "    except:\n",
    "        my_compiler=[my_platform, \"Unavailable\", []]\n",
    "\n",
    "    if my_compiler:\n",
    "        return my_compiler\n",
    "    else:\n",
    "        return [\"Unknown\", \"Unavailable\", []]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eaf207b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiler_cmd = c_compiler_cmd(\"optimized\")\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=categorize_func, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            sample_program = gr.Radio([\"categorize_items\"], label=\"Sample program\", value=\"categorize_items\")\n",
    "            model = gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\", \"Qwen2\", \"Gemma\"], label=\"Select model\", value=\"Claude\")\n",
    "        with gr.Column():\n",
    "            architecture = gr.Radio([compiler_cmd[0]], label=\"Architecture\", interactive=False, value=compiler_cmd[0])\n",
    "            compiler = gr.Radio([compiler_cmd[1]], label=\"Compiler\", interactive=False, value=compiler_cmd[1])\n",
    "    with gr.Row():\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        if not compiler_cmd[1] == \"Unavailable\":\n",
    "            cpp_run = gr.Button(\"Run C++\")\n",
    "        else:\n",
    "            cpp_run = gr.Button(\"No compiler to run C++\", interactive=False)\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "\n",
    "    sample_program.change(select_sample_program, inputs=[sample_program], outputs=[python])\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
